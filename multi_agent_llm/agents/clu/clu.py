import threading
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Tuple

from pydantic import BaseModel, Field
from rich.color import Color
from rich.console import Console
from rich.panel import Panel
from rich.style import Style
from rich.table import Table
from rich.theme import Theme

my_theme = Theme(
    {
        "info": Style(color=Color.from_rgb(50, 150, 200)),
        "warning": Style(color=Color.from_rgb(200, 150, 50)),
        "error": Style(color=Color.from_rgb(200, 50, 50)),
        "highlight": Style(color=Color.from_rgb(100, 200, 100)),
    }
)

console = Console(theme=my_theme)

from .kmu import KnowledgeManagementUnit


class CompositeLearnUnit:
    def __init__(
        self,
        llm,
        main_goal: str,
        name: Optional[str] = None,
        general_main_goal: Optional[str] = None,
        prompt_main_goal: Optional[str] = None,
        storage_goal: Optional[str] = None,
        retrieval_goal: Optional[str] = None,
        compress_knowledge: bool = True,
    ):
        self.llm = llm
        self.main_goal = main_goal
        self.name = name
        self.compress = compress_knowledge

        # Set default goals if not provided
        self.storage_goal = (
            storage_goal
            or f"Store abstract knowledge and strategies that can be applied to a wide range of tasks and problem-solving scenarios do the main goal - Main goal: {main_goal} in the best way possible. Encourage storing knowledge that can be used in future tasks and not just for the current task."
        )
        self.retrieval_goal = (
            retrieval_goal
            or f"Retrieve relevant abstract knowledge and strategies that can be applied to the current task or query.based on the main role - Main goal: {main_goal}"
        )
        self.general_main_goal = (
            general_main_goal
            or f"Develop and maintain a versatile knowledge base of abstract concepts, patterns, and problem-solving approaches with the main goal: {main_goal} in mind."
        )
        self.prompt_main_goal = (
            prompt_main_goal
            or "Refine strategies for generating effective prompts that can guide reasoning across various types of tasks.Extract relevant information needed to construct high-quality prompts based on the feedback from answering specific task-related query with a given old prompt. - Main goal:  {main_goal}"
        )

        self.general_kmu = KnowledgeManagementUnit(
            llm,
            main_goal=self.general_main_goal,
            storage_goal=self.storage_goal,
            retrieval_goal=self.retrieval_goal,
            name=(
                "GeneralKMU_" + self.name
                if self.name is not None
                else "GeneralKMU_{name}".format(name=str(uuid.uuid4()))
            ),
        )
        self.prompt_kmu = KnowledgeManagementUnit(
            llm,
            main_goal=self.prompt_main_goal,
            storage_goal=self.storage_goal,
            retrieval_goal=self.retrieval_goal,
            name=(
                "PromptKMU_" + self.name
                if self.name is not None
                else "PromptKMU_{name}".format(name=str(uuid.uuid4()))
            ),
        )

    def _log_step(self, agent_name: str, input_data: str, output_data: str):
        table = Table(title=f"[bold]{agent_name}[/bold]", show_header=False, box=None)
        table.add_row("[cyan]Input:[/cyan]", str(input_data))
        table.add_row("[green]Output:[/green]", str(output_data))
        console.print(Panel(table, expand=False))

    def _prompt_meta_prompt_agent(
        self, task: str, prompt_knowledge: str, verbose: bool = False
    ) -> str:
        system_prompt = f"""You are the Meta-Prompt Agent in the Composite Learning Unit.
        Main Goal: {self.prompt_main_goal}
        Your task is to generate a detailed, task-specific prompt for the Operational Agent based on the given task and retrieved prompt knowledge.
        This prompt should guide the Operational Agent in effectively using the general knowledge to complete the task."""

        user_prompt = f"Task: {task}\nPrompt Knowledge: {prompt_knowledge}\nGenerate a detailed task-specific prompt:"

        class MetaPromptAgentOutput(BaseModel):
            prompt: str = Field(
                ...,
                description="The detailed task-specific prompt generated by the Meta-Prompt Agent",
            )

        query = self.llm.format_prompt(
            system_prompt=system_prompt, user_prompt=user_prompt
        )
        result = self.llm.generate(query, schema=MetaPromptAgentOutput)

        if verbose:
            self._log_step("Meta-Prompt Agent", query, result.prompt)

        return result.prompt

    def _prompt_operational_agent(
        self,
        task: str,
        general_knowledge: str,
        task_prompt: str,
        schema: BaseModel,
        verbose: bool = False,
    ) -> Any:
        system_prompt = f"""You are the Operational Agent in the Composite Learning Unit.
        Main Goal: {self.main_goal}
        Your task is to process the given task using the provided general knowledge and following the detailed task-specific prompt.
        Before answering, first think step by step and explain the reasoning behind the answer and how the which parts of the general knowledge and prompt knowledge were used to arrive at the answer. Give detailed reasoning and explanation.
        """

        user_prompt = f"Task: {task}\nGeneral Knowledge: {general_knowledge}\nDetailed Task-Specific Prompt: {task_prompt}\nExecute the task:"

        class OperationalAgentOutput(BaseModel):
            explanation: str = Field(
                ..., description="Detailed explanation of the response"
            )
            answer: schema = Field(
                ...,
                description="The response answer generated by the Operational Agent",
            )

        query = self.llm.format_prompt(
            system_prompt=system_prompt, user_prompt=user_prompt
        )
        result = self.llm.generate(query, schema=OperationalAgentOutput)

        if verbose:
            self._log_step("Operational Agent", query, str(result))

        return result

    def reason(
        self,
        query: str,
        schema: BaseModel,
        verbose: bool = False,
        _capture_knowledge: bool = False,
    ) -> Any:
        if verbose:
            console.print(
                "[info]Starting Reasoning Process[/info]",
            )

        # Retrieve general knowledge
        general_knowledge = self.general_kmu.retrieve(query)
        if verbose:
            self._log_step("General KMU Retrieval", query, general_knowledge)

        # Retrieve prompt knowledge
        prompt_knowledge = self.prompt_kmu.retrieve(query)
        if verbose:
            self._log_step("Prompt KMU Retrieval", query, prompt_knowledge)

        # Generate task-specific prompt
        task_prompt = self._prompt_meta_prompt_agent(query, prompt_knowledge, verbose)

        # Execute task using Operational Agent
        result = self._prompt_operational_agent(
            query, general_knowledge, task_prompt, schema, verbose
        )

        if verbose:
            console.print(
                "[info]Reasoning Process Completed[/info]",
            )

        if _capture_knowledge:
            return result, general_knowledge, prompt_knowledge
        else:
            return result

    def save_general_knowledge(self, knowledge: str, compress: bool = True) -> str:
        return self.general_kmu.save(knowledge, compress)

    def save_prompt_knowledge(self, knowledge: str, compress: bool = True) -> str:
        return self.prompt_kmu.save(knowledge, compress)

    def _prompt_comparison_agent(
        self,
        query: str,
        main_goal: str,
        generated_output: Any,
        expected_output: Any,
        verbose: bool = False,
    ) -> Tuple[bool, str]:
        system_prompt = f"""You are the Comparison Agent in the Composite Learning Unit.
        Main Goal: {main_goal}
        Your task is to compare the generated output with the expected output, considering the query and main goal.
        Determine if they are equivalent, either verbatim or semantically, based on the context of the task.
        
        - Determine if the response matches the expected output exactly
        - If it does not match exactly see if it matches in meaning and intent clearly and unambiguously.
        - Provide an explanation of your comparison, highlighting similarities and differences
        - Return a boolean indicating whether the response is correct and an explanation
        - Do not speculate what might be the reason for the difference, focus on the comparison itself.
        """

        user_prompt = f"""Query: {query}
        Generated Output: {generated_output}
        Expected Output: {expected_output}
        Are these outputs equivalent? Provide a boolean result and a detailed explanation.
        """

        class ComparisonAgentOutput(BaseModel):
            is_equivalent: bool = Field(
                ..., description="Boolean indicating if the outputs are equivalent"
            )
            explanation: str = Field(
                ..., description="Detailed explanation of the equivalence decision"
            )

        query = self.llm.format_prompt(
            system_prompt=system_prompt, user_prompt=user_prompt
        )
        result = self.llm.generate(query, schema=ComparisonAgentOutput)

        if verbose:
            self._log_step(
                "Comparison Agent",
                query,
                f"Equivalent: {result.is_equivalent}\nExplanation: {result.explanation}",
            )

        return result.is_equivalent, result.explanation

    def _prompt_feedback_agent(
        self,
        is_positive: bool,
        query: str,
        main_goal: str,
        generated_output: Any,
        expected_output: Any,
        comparison_explanation: str,
        general_knowledge: str,
        prompt_knowledge: str,
        verbose: bool = False,
        max_feedback: int = 5,
    ) -> Dict[str, str]:
        agent_type = "Positive" if is_positive else "Negative"

        if is_positive:
            system_prompt = f"""You are the Positive Feedback Agent in the Composite Learning Unit.
            Main Goal: {main_goal}
            Begin by enclosing all thoughts within <thinking> tags, focusing on identifying the positive elements of the knowledge base that led to the successful outcome. Explore multiple perspectives to understand the strengths of the current approach.

            Break down your positive feedback into clear, reinforcing actions within <step> tags. Start with a 20-step budget, focusing on:

            Identifying and explaining which elements of the general knowledge base contributed to the successful outcome.
            Analyzing how the prompt knowledge effectively guided the reasoning process.
            Identifying key elements in the reasoning that led to the correct answer.
            Suggesting how this success can be replicated in future tasks.
            Use <count> tags after each step to indicate the remaining feedback budget, stopping when it reaches 0. Continuously adapt your feedback based on reflections to maximize the reinforcement of successful strategies.

            Regularly evaluate your reinforcement strategy using <reflection> tags. Be critical and honest about what worked and why. Assign a quality score between 0.0 and 1.0 using <reward> tags after each reflection:

            0.8+: Continue reinforcing the current approach.
            0.5-0.7: Consider enhancing some elements to strengthen future performance.
            Below 0.5: Re-evaluate the strengths identified and explore different ways to replicate success.
            If unsure or if the reward score is low, adjust your feedback to enhance understanding and explain your revised approach within <thinking> tags.

            For each successful element identified, enclose your findings within <success> tags, clearly specifying the contributing factors. Provide actionable suggestions for leveraging these elements in future tasks within <strategy> tags to ensure consistency and improvement.

            If there are multiple strengths or positive aspects worth exploring further, outline each of them individually within <thinking> tags, comparing their relative impact on the successful outcome in <reflection> tags to understand their importance and contribution.

            Continuously aim to provide constructive feedback that encourages:

            Building upon the identified successes.
            Reinforcing effective components of the reasoning process.
            Replicating the strategies that led to the correct answer in future scenarios.
            Synthesize the overall positive feedback within <summary> tags, summarizing key strengths, their impact, and actionable recommendations for future success.

            Conclude with a final reflection within <final_reflection> tags, discussing the key elements of the success, how they can be consistently applied, and areas for potential enhancement. Assign a final reward score based on the overall reinforcement process.
            
            **IMPORTATNT:** Do all these with the main goal in mind: {main_goal}.
            Give a detailed list of no more than {max_feedback} feedback each for improving general knowledge and prompt knowledge."""
        else:
            system_prompt = f"""You are the Negative Feedback Agent in the Composite Learning Unit.
            Main Goal: {main_goal}
            
            Begin by enclosing all thoughts within <thinking> tags, focusing on identifying gaps and inaccuracies in the knowledge base that led to the incorrect outcome. Explore multiple perspectives to thoroughly assess the issue.

            Break down your feedback into clear corrective actions within <step> tags. Start with a 20-step budget, focusing on:

            Identifying gaps or inaccuracies in the knowledge bases.
            Analyzing how the knowledge may have misguided the reasoning process.
            Evaluating why the response does not match the expected outcome.
            Identifying key elements in the reasoning that led to the incorrect answer.
            Suggesting specific changes and improvements to the knowledge base.
            Providing strategies to avoid similar mistakes in the future.
            After each step, use <count> tags to indicate the remaining feedback budget, stopping when reaching 0. Adjust your strategy continuously based on intermediate reflections, adapting your feedback to address newly identified issues or potential corrections.

            Regularly evaluate your corrective approach using <reflection> tags. Be critical and honest about the effectiveness of each corrective action. Assign a quality score between 0.0 and 1.0 using <reward> tags after each reflection:

            0.8+: Continue the current corrective approach.
            0.5-0.7: Consider minor adjustments to the feedback or strategy.
            Below 0.5: Seriously consider backtracking and trying a different corrective approach.
            If unsure or if the reward score is low, backtrack and try a different feedback strategy, explaining your reasoning and new direction within <thinking> tags.

            For each identified gap or issue, clearly enclose your findings within <gap> tags. Suggest potential changes or additions to the knowledge base within <strategy> tags, ensuring your recommendations are specific and actionable.

            If multiple feedback strategies are viable, explore each one individually, comparing their effectiveness within <reflection> tags to decide the best corrective course. Use <thinking> tags as a scratchpad to outline alternative approaches for addressing gaps in the knowledge or reasoning.

            Continuously aim to provide constructive and diverse feedback that targets specific aspects, including:

            Concrete improvements to the knowledge base.
            Strategies to prevent similar mistakes in future iterations.
            Key elements of the reasoning that need to be corrected.
            Synthesize all your feedback into a concise summary within <summary> tags, including the main corrective points and the rationale behind suggested improvements.

            Conclude with a final reflection within <final_reflection> tags, discussing the effectiveness of the corrective process, challenges faced, and how the feedback provided might impact future performance. Assign a final reward score to the overall feedback process.
            **IMPORTATNT:** Do all these with the main goal in mind: {main_goal}.
            Give a detailed list of no more than {max_feedback} feedback each for improving general knowledge and prompt knowledge."""

        user_prompt = f"""Query: {query}
        Generated Output: {generated_output}
        Expected Output: {expected_output}
        Comparison Explanation: {comparison_explanation}
        General Knowledge Used: {general_knowledge}
        Prompt Knowledge Used: {prompt_knowledge}
        Provide detailed, through and targeted feedback for improving general knowledge and prompt knowledge:"""

        class FeedbackAgentOutput(BaseModel):
            thinking_process: str = Field(
                ..., description="Thinking process of the agent"
            )
            general_knowledge_feedback: List[str] = Field(
                ..., description="Feedback for improving the general knowledge base"
            )
            prompt_knowledge_feedback: List[str] = Field(
                ..., description="Feedback for improving the prompt knowledge"
            )

        query = self.llm.format_prompt(
            system_prompt=system_prompt, user_prompt=user_prompt
        )
        result = self.llm.generate(query, schema=FeedbackAgentOutput)

        if verbose:
            self._log_step(
                f"{agent_type} Feedback Agent",
                query,
                f"General Knowledge Feedback: {result.general_knowledge_feedback}\nPrompt Knowledge Feedback: {result.prompt_knowledge_feedback}",
            )

        return {
            "thinking_process": result.thinking_process,
            "general_knowledge_feedback": result.general_knowledge_feedback,
            "prompt_knowledge_feedback": result.prompt_knowledge_feedback,
        }

    def train(
        self,
        x: Any,
        y: Optional[Any] = None,
        schema: BaseModel = None,
        verbose: bool = False,
    ) -> Dict[str, Any]:
        if verbose:
            console.print("[info]Starting Training Process[/info]")

        # Perform reasoning and capture knowledge used
        generated_output, general_knowledge_used, prompt_knowledge_used = self.reason(
            x, schema, verbose, _capture_knowledge=True
        )

        if y is None:
            if verbose:
                console.print(
                    "[warning]No expected output provided. Skipping comparison and feedback.[/warning]"
                )
            return {"generated_output": generated_output}

        # Compare generated output with expected output
        is_equivalent, comparison_explanation = self._prompt_comparison_agent(
            x, self.main_goal, generated_output, y, verbose
        )

        # Generate feedback based on comparison results and knowledge used
        feedback = self._prompt_feedback_agent(
            is_equivalent,
            x,
            self.main_goal,
            generated_output,
            y,
            comparison_explanation,
            general_knowledge_used,
            prompt_knowledge_used,
            verbose,
        )

        # Align and save new knowledge in parallel
        with ThreadPoolExecutor(max_workers=2) as executor:
            general_future = executor.submit(
                self._align_and_save_knowledge,
                "general",
                f"Thinking process : {feedback['thinking_process']}"
                + str(feedback["general_knowledge_feedback"]),
                general_knowledge_used,
                x,
                verbose,
            )
            prompt_future = executor.submit(
                self._align_and_save_knowledge,
                "prompt",
                f"Thinking process : {feedback['thinking_process']}"
                + str(feedback["prompt_knowledge_feedback"]),
                prompt_knowledge_used,
                x,
                verbose,
            )

            general_saved_ids = general_future.result()
            prompt_saved_ids = prompt_future.result()

        if verbose:
            console.print("[info]Training Process Completed[/info]")

        return {
            "generated_output": generated_output,
            "is_equivalent": is_equivalent,
            "comparison_explanation": comparison_explanation,
            "feedback": feedback,
            "new_general_knowledge_ids": general_saved_ids,
            "new_prompt_knowledge_ids": prompt_saved_ids,
        }

    def _prompt_knowledge_alignment_agent(
        self,
        knowledge_type: str,
        feedback: str,
        existing_knowledge: str,
        query: str,
        main_goal: str,
        verbose: bool = False,
        max_knowldge: int = 5,
    ) -> List[str]:
        system_prompt = f"""You are the Knowledge Alignment Agent for {knowledge_type} knowledge in the Composite Learning Unit.
        Main Goal: {main_goal}
        Your task is to process the feedback and existing knowledge to generate a comprehensive list of knowledge entries that will replace the current knowledge base.
        This list should include:
        1. Modified versions of existing knowledge entries that address the feedback.
        2. New knowledge entries that fill gaps identified in the feedback.
        3. Existing knowledge entries that remain relevant and accurate.
        4. Exclude any existing knowledge that is no longer relevant or accurate based on the feedback.
        5. Always collect diverse list of knowledge entries to ensure comprehensive coverage that is relevant to the main goal.
        6. Favour adding targeted short and relevant knowledge entries over long and verbose entries.
        7. Never memorize the tasks, only extract and save the general and relevant information that can be used in future tasks.
        8. If something methods are tired and is wrong or did not work, always keep the record of that and do not repeat the same mistake again.
        Begin by processing all feedback within <thinking> tags, focusing on extracting, refining, and aligning the knowledge to support the {main_goal} effectively. Carefully consider how feedback and existing knowledge entries relate to each other and how modifications will meet the requirements of {knowledge_type} knowledge.

        Break down your task into clear knowledge alignment steps within <step> tags. Start with a 20-step budget, focusing on:

        Identifying existing knowledge entries that need modification based on the feedback. Include these as modified versions in the new knowledge list.
        Extracting new knowledge entries that are necessary to fill gaps identified in the feedback. Highlight these entries for addition.
        Evaluating existing knowledge entries to determine which are still relevant and accurate.
        Excluding knowledge entries that no longer serve their purpose or have become outdated based on the feedback.
        After each step, use <count> tags to show the remaining budget, stopping once it reaches 0. Continuously adjust your strategy based on reflections as you proceed.

        Regularly evaluate your knowledge alignment process using <reflection> tags. Be critical and honest about the quality and completeness of the extracted knowledge. Assign a quality score between 0.0 and 1.0 using <reward> tags after each reflection:

        0.8+: Continue with the current knowledge alignment strategy.
        0.5-0.7: Consider minor adjustments to improve coverage or clarity.
        Below 0.5: Seriously reconsider your approach to extracting and modifying knowledge, and explain the revised approach within <thinking> tags.
        If unsure or if the reward score is low, backtrack and try a different alignment strategy, elaborating on the new adjustments within <thinking> tags.

        For each modified or newly added knowledge entry, use <entry> tags to clearly mark the content. Ensure that each entry is short, relevant, and targeted, rather than verbose. Focus on maintaining comprehensive yet concise knowledge coverage to meet the main goal.

        After analyzing existing knowledge, use <retain> tags to list entries that remain valid and useful, and use <exclude> tags to indicate those that should be discarded.

        Throughout your process, collect a diverse set of knowledge entries to ensure comprehensive coverage of {knowledge_type} knowledge relevant to achieving the {main_goal}. Avoid memorizing tasks; instead, focus on saving general and relevant information that can be used for future tasks, using <save> tags for storing this key information.

        For any method that proved incorrect or ineffective, clearly note this using <do_not_repeat> tags to avoid making the same mistake again in future tasks.

        When all modifications, additions, and exclusions are complete, synthesize the final comprehensive list within <summary> tags. Ensure that the new knowledge base is complete, up-to-date, and aligned to meet all aspects of the main goal.

        Conclude with a final reflection within <final_reflection> tags, discussing the completeness and accuracy of the aligned knowledge, the challenges faced, and any considerations for maintaining or improving the knowledge base in future tasks. Assign a final reward score to evaluate the entire knowledge alignment process
        
        Remember, the list you provide will completely replace the existing knowledge, so ensure it is comprehensive and addresses all aspects of the {knowledge_type} knowledge needed to achieve the main goal.
        
        **IMPORTANT** : After this based on your reasoning, give back a list of new knowledge entries that will replace the existing knowledge base. Give no more than {max_knowldge} new knowledge entries, combine the new knowledge entries without leaving any details to make the list of {max_knowldge} entries.
        """

        user_prompt = f"""Query: {query}
        Existing {knowledge_type} Knowledge: {existing_knowledge}
        Feedback for {knowledge_type} Knowledge: {feedback}
        Generate a comprehensive list of knowledge entries that will replace the existing {knowledge_type} knowledge base:"""

        class KnowledgeAlignmentOutput(BaseModel):
            thinking_process: str = Field(
                ..., description="Thinking process of the agent"
            )
            new_knowledge: List[str] = Field(
                ...,
                description=f"Final list of {knowledge_type} knowledge entries to replace the existing knowledge base",
            )

        query = self.llm.format_prompt(
            system_prompt=system_prompt, user_prompt=user_prompt
        )
        result = self.llm.generate(query, schema=KnowledgeAlignmentOutput)

        if verbose:
            self._log_step(
                f"{knowledge_type.capitalize()} Knowledge Alignment Agent",
                query,
                f"New Knowledge Entries: {result.new_knowledge}",
            )

        return result.new_knowledge

    def _align_and_save_knowledge(
        self,
        knowledge_type: str,
        feedback: str,
        existing_knowledge: str,
        query: str,
        verbose: bool = False,
    ) -> List[str]:
        new_knowledge_entries = self._prompt_knowledge_alignment_agent(
            knowledge_type, feedback, existing_knowledge, query, self.main_goal, verbose
        )

        if knowledge_type == "general":
            kmu = self.general_kmu
        else:
            kmu = self.prompt_kmu

        # Get the IDs of the existing knowledge entries
        existing_ids = kmu.collection.get()["ids"]

        # Replace the existing knowledge with the new entries
        saved_ids = kmu.replace_knowledge(
            existing_ids, new_knowledge_entries, compress=self.compress, verbose=verbose
        )

        return saved_ids

    def print_knowledge(self, verbose: bool = False):
        console.print("[bold]General Knowledge:[/bold]")
        self.general_kmu.print_knowledge(verbose)

        console.print("\n[bold]Prompt Knowledge:[/bold]")
        self.prompt_kmu.print_knowledge(verbose)


def example_usage():
    class OperationalAgentOutput(BaseModel):
        response: str = Field(
            ..., description="The response generated by the Operational Agent"
        )

    # Define a mock LLM class for demonstration
    class MockLLM:
        def generate(self, prompt, schema):
            # This is a simplistic mock. In reality, you'd use a real LLM here.
            return schema(
                **{
                    field: f"Mocked {field} based on: {prompt}"
                    for field in schema.__fields__
                }
            )

        def format_prompt(self, system_prompt, user_prompt):
            return f"{system_prompt}\n{user_prompt}"

    # Initialize the CLU
    llm = MockLLM()
    clu = CompositeLearnUnit(
        llm,
        main_goal="Provide accurate and helpful information across various domains",
        general_main_goal="Maintain a comprehensive knowledge base for general reasoning and problem-solving",
        prompt_main_goal="Develop effective prompting strategies for diverse tasks and contexts",
        storage_goal="Store knowledge efficiently, emphasizing clarity and relevance",
        retrieval_goal="Retrieve the most pertinent information for each specific task or query",
    )

    # Manually add some initial knowledge to the KMUs
    clu.save_general_knowledge(
        "Climate change is causing global temperatures to rise, leading to various environmental impacts."
    )
    clu.save_prompt_knowledge(
        "When explaining complex topics, start with a simple analogy before providing technical details."
    )

    # Perform training
    query = "Explain the potential impacts of climate change on marine ecosystems"
    expected_output = "Climate change affects marine ecosystems through ocean warming, acidification, and sea level rise, leading to habitat loss and species migration."
    training_result = clu.train(
        query, expected_output, OperationalAgentOutput, verbose=True
    )

    console.print("[bold green]Training Result:[/bold green]", training_result)


if __name__ == "__main__":
    example_usage()
